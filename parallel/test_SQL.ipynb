{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_PTS = 4\n",
    "EPSILON = 900\n",
    "X_UNIT, Y_UNIT = 5000, 5000\n",
    "SPAN_MARGIN = EPSILON/2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('x', IntegerType(), True), StructField('y', IntegerType(), True)])\n",
      "+-----+-----+\n",
      "|    x|    y|\n",
      "+-----+-----+\n",
      "|54620|43523|\n",
      "|52694|42750|\n",
      "|53253|43024|\n",
      "+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read\n",
    "dataset = spark.read.text(\"/Users/apple/spark/data/project/A-sets/a1.txt\")\n",
    "# for each row, remove the space and split the string into a list\n",
    "dataset = dataset.select(split(trim(dataset.value), '\\\\s+').alias('coor'))\n",
    "dataset = dataset.select([col('coor')[0].cast('int').alias('x'), col('coor')[1].cast('int').alias('y')])\n",
    "\n",
    "# only keep the first 800 rows\n",
    "dataset = dataset.limit(800)\n",
    "print(dataset.schema)\n",
    "dataset.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = dataset.agg(min('x'), max('x')).first()\n",
    "y_min, y_max = dataset.agg(min('y'), max('y')).first()\n",
    "\n",
    "x_grid = list(range(x_min, x_max + X_UNIT, X_UNIT))\n",
    "y_grid = list(range(y_min, y_max + Y_UNIT, Y_UNIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+-------+-----+\n",
      "|area_id|x_start|x_end|y_start|y_end|\n",
      "+-------+-------+-----+-------+-----+\n",
      "|      0|  35425|41327|  38905|44807|\n",
      "|      1|  35425|41327|  43905|49807|\n",
      "|      2|  35425|41327|  48905|54807|\n",
      "+-------+-------+-----+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "area_list = []\n",
    "area_cnt = 0\n",
    "for x_idx, x_start in enumerate(x_grid[:-1]):\n",
    "    for y_idx, y_start in enumerate(y_grid[:-1]):\n",
    "        x_start_adjusted = x_start - SPAN_MARGIN\n",
    "        y_start_adjusted = y_start - SPAN_MARGIN\n",
    "        x_end_adjusted = x_grid[x_idx+1] + SPAN_MARGIN\n",
    "        y_end_adjusted = y_grid[y_idx+1] + SPAN_MARGIN\n",
    "        area_list.append([area_cnt, x_start_adjusted, x_end_adjusted, y_start_adjusted, y_end_adjusted])\n",
    "        area_cnt += 1\n",
    "assert len(area_list) == (len(x_grid)-1)*(len(y_grid)-1)\n",
    "\n",
    "# organize the area into a dataframe, and add an area_id (start from 0)\n",
    "area_df = spark.createDataFrame(area_list, ['area_id', 'x_start', 'x_end', 'y_start', 'y_end'])\n",
    "# convert all columns to integer\n",
    "area_df = area_df.select([col(c).cast('int') for c in area_df.columns])\n",
    "\n",
    "area_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+\n",
      "|    x|    y|area_id|\n",
      "+-----+-----+-------+\n",
      "|37900|43700|      0|\n",
      "|38172|42792|      0|\n",
      "|38870|44459|      0|\n",
      "+-----+-----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split the dataset according to the area\n",
    "dataset_with_area = dataset.crossJoin(area_df).filter((col('x') >= col('x_start')) & (col('x') <= col('x_end')) & (col('y') >= col('y_start')) & (col('y') <= col('y_end'))).select('x', 'y', 'area_id')\n",
    "dataset_with_area.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may be used later\n",
    "plt.scatter(dataset.select('x').collect(), dataset.select('y').collect(), s=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
